{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUQyNBeNW5WwbnlhCrKMpm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panditamey1/NLP_transformers/blob/main/transformers_pytorch_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self attention\n"
      ],
      "metadata": {
        "id": "vn8ulUZMZ0F4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key, Query, Value \n",
        "- Create matrices for KQV\n",
        " "
      ],
      "metadata": {
        "id": "qooexvOlvQCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "from torch import nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "yzTCG9iVwoZe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = 4\n",
        "d = 12 # model dimensions\n",
        "\n",
        "Wk = np.random.randn(d,d)\n",
        "Wq = np.random.randn(d,d)\n",
        "Wv = np.random.randn(d,d)\n",
        "\n",
        "Wk.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKduxupwvYgF",
        "outputId": "fb48649f-5913-4fcd-80bd-e95fe1dfa397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.randn(t,d)  # t = token size"
      ],
      "metadata": {
        "id": "Ik2DzRnGxADa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-guacgexRny",
        "outputId": "5ad68669-fbb3-4768-fe2a-af2e30224490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- take dot product of (Q*K.T/np.sqrt(d) )* V"
      ],
      "metadata": {
        "id": "Hetf6QPgxc-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q = np.dot(X, Wq)\n",
        "K = np.dot(X, Wk)\n",
        "\n",
        "K.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zku8qhhFxWF6",
        "outputId": "4fd16fe8-9f9b-44f9-df5d-ecfd0ab4d01a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## dot product of q and k\n",
        "\n",
        "QK = np.dot(Q,K.T)\n"
      ],
      "metadata": {
        "id": "PR23S76xykgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QK = QK / np.sqrt(d)"
      ],
      "metadata": {
        "id": "ww-g18Fpyxex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Softmax\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    print(e_x.shape)\n",
        "    res =  e_x / e_x.sum(axis=0) # only difference\n",
        "    print(res.shape)\n",
        "    return res\n",
        "    "
      ],
      "metadata": {
        "id": "M1u6W0dIy4OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_QK = softmax(QK)\n",
        "QK.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ2S9x_94rnS",
        "outputId": "ad92233a-9b6d-4bdb-a422-3f945f601d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V = np.dot(X, Wv)"
      ],
      "metadata": {
        "id": "ku7BoWfc42iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvJUG_b65DRx",
        "outputId": "29f82565-ab9d-475d-eb7f-66ce652f8554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QKV = np.dot(scaled_QK, V)"
      ],
      "metadata": {
        "id": "Tkjy0r0C0CTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QKV.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LceO5GXk0N_v",
        "outputId": "45ec67d6-fd79-4d74-dfe5-e4f1b14eb1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jK2EEjkK5LcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create function for self attention"
      ],
      "metadata": {
        "id": "5BvXZ21m5kX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_self_attention(query, key, value):\n",
        "  key_dim = key.shape[1]\n",
        "  QK = np.dot(query,key.T)\n",
        "  #/np.sqrt(key_dim)\n",
        "  print(\"---\")\n",
        "  print(QK.shape)\n",
        "  #sf = nn.Softmax( dim=1)\n",
        "  scaled_QK = softmax(QK)\n",
        "  print(scaled_QK.shape)\n",
        "  QKV = np.dot(scaled_QK, value)\n",
        "  print(QKV.shape)\n",
        "  return QKV, scaled_QK"
      ],
      "metadata": {
        "id": "yzF3cQ4F5qhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_self_attention(Q, K, V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sA4a3K2-62_p",
        "outputId": "0729ae5a-c71d-4413-c555-2f9d54048f4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 6.59297357e-47,  6.11623018e-47,  2.09227010e-46,\n",
              "          1.90797248e-46, -1.28153513e-47, -1.09696246e-46,\n",
              "          2.74316935e-47, -6.58638251e-47, -1.39020227e-46,\n",
              "          2.01060110e-46,  7.13266126e-47, -1.89522458e-46],\n",
              "        [ 5.41346164e-40,  4.36153826e-40,  1.57390668e-39,\n",
              "          1.45347501e-39, -7.28558597e-41, -8.25978126e-40,\n",
              "          1.33804646e-40, -5.33975408e-40, -1.09072760e-39,\n",
              "          1.56123496e-39,  5.51334414e-40, -1.46958532e-39],\n",
              "        [-8.43764302e+00,  8.77144346e+00, -9.46910444e-01,\n",
              "          2.97250628e+00,  2.36695398e+00, -3.98113213e+00,\n",
              "         -2.28811882e+00,  4.45762186e+00,  4.50286565e+00,\n",
              "          1.52476604e+00,  2.72580162e+00, -3.35865925e-01],\n",
              "        [ 7.66125828e-32, -3.30838028e-32,  8.32463766e-32,\n",
              "          1.97255430e-32, -6.90215186e-32, -3.29037147e-31,\n",
              "         -6.06296034e-32,  1.18175743e-31,  1.12543985e-31,\n",
              "          4.17384358e-32,  1.40384174e-31, -3.48219463e-32]]),\n",
              " array([[9.02178750e-65, 3.96121058e-47, 5.14271272e-65, 9.66142092e-49],\n",
              "        [6.70014723e-63, 3.04006206e-40, 3.70511420e-48, 2.96643146e-54],\n",
              "        [1.32347586e-49, 6.87231950e-52, 1.00000000e+00, 3.95675085e-14],\n",
              "        [7.70681336e-32, 6.22393931e-53, 3.51300617e-43, 1.95615720e-71]]))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi head attention"
      ],
      "metadata": {
        "id": "g2ar1x7v6_A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 3\n",
        "embed_dim = 4\n",
        "\n",
        "queries = np.random.rand(seq_len, embed_dim)\n",
        "keys = np.random.rand(seq_len, embed_dim)\n",
        "values = np.random.rand(seq_len, embed_dim)\n",
        "\n",
        "print(\"Queries:\\n\", queries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CfIr4CFjXb3",
        "outputId": "6c3b995f-2512-4ac5-b843-e8d2815f7261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries:\n",
            " [[0.28484049 0.03688695 0.60956433 0.50267902]\n",
            " [0.05147875 0.27864646 0.90826589 0.23956189]\n",
            " [0.14489487 0.48945276 0.98565045 0.24205527]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output, attn_weights = scaled_self_attention(queries, keys, values)\n",
        "\n",
        "print(\"Output\\n\", output, \"\\n\")\n",
        "print(\"Weights\\n\", attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9fmX9ksjhKf",
        "outputId": "c2e6c600-b144-4a9b-c746-3652065202a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 3)\n",
            "(3, 3)\n",
            "Output\n",
            " [[0.34046052 0.3583313  0.65012393 0.09111357]\n",
            " [0.40349096 0.37504672 0.71582375 0.1059299 ]\n",
            " [0.49985446 0.47074605 0.89351946 0.13143174]] \n",
            "\n",
            "Weights\n",
            " [[0.3254204  0.28426828 0.26285964]\n",
            " [0.29641855 0.31901007 0.33004653]\n",
            " [0.37816105 0.39672165 0.40709382]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "seq_len = 3\n",
        "embed_dim = 12\n",
        "num_heads = 3\n",
        "head_dim = embed_dim // num_heads"
      ],
      "metadata": {
        "id": "9LUB1quz7qhB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.rand(batch_size, seq_len, embed_dim).round(1)\n",
        "print(\"Input shape: \", x.shape, \"\\n\")\n",
        "print(\"Input:\\n\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz3YSeXMkAQe",
        "outputId": "341a4f74-e7a2-482a-b7ff-558b4e850c42"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  (1, 3, 12) \n",
            "\n",
            "Input:\n",
            " [[[0.4 1.  0.7 0.6 0.2 0.2 0.1 0.9 0.6 0.7 0.  1. ]\n",
            "  [0.8 0.2 0.2 0.2 0.3 0.5 0.4 0.3 0.6 0.1 0.3 0.4]\n",
            "  [0.5 0.8 0.2 0.5 0.6 0.  0.6 0.2 0.1 0.9 1.  0.8]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The query weights for each head.\n",
        "wq0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wq1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wq2 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "\n",
        "# The key weights for each head. \n",
        "wk0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wk1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wk2 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "\n",
        "# The value weights for each head.\n",
        "wv0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wv1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wv2 = np.random.rand(embed_dim, head_dim).round(1)"
      ],
      "metadata": {
        "id": "aAiaF8c2kAz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The three sets of query weights (one for each head):\")\n",
        "print(\"wq0:\\n\", wq0)\n",
        "print(\"wq1:\\n\", wq1)\n",
        "print(\"wq2:\\n\", wq1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNoJdAvmkEUX",
        "outputId": "b4737103-687d-48d5-afce-da6271800719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The three sets of query weights (one for each head):\n",
            "wq0:\n",
            " [[0.9 0.6 0.3 0.3]\n",
            " [0.7 0.9 0.9 0.8]\n",
            " [0.6 0.1 0.2 0.9]\n",
            " [0.6 0.  0.1 0.7]]\n",
            "wq1:\n",
            " [[0.  0.2 0.5 0.7]\n",
            " [0.7 0.2 0.7 0.2]\n",
            " [0.3 0.7 0.6 0.8]\n",
            " [0.7 0.6 0.1 0.4]]\n",
            "wq2:\n",
            " [[0.  0.2 0.5 0.7]\n",
            " [0.7 0.2 0.7 0.2]\n",
            " [0.3 0.7 0.6 0.8]\n",
            " [0.7 0.6 0.1 0.4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Geneated queries, keys, and values for the first head.\n",
        "q0 = np.dot(x, wq0)\n",
        "k0 = np.dot(x, wk0)\n",
        "v0 = np.dot(x, wv0)\n",
        "\n",
        "# Geneated queries, keys, and values for the second head.\n",
        "q1 = np.dot(x, wq1)\n",
        "k1 = np.dot(x, wk1)\n",
        "v1 = np.dot(x, wv1)\n",
        "\n",
        "# Geneated queries, keys, and values for the third head.\n",
        "q2 = np.dot(x, wq2)\n",
        "k2 = np.dot(x, wk2)\n",
        "v2 = np.dot(x, wv2)"
      ],
      "metadata": {
        "id": "_2A-4jKUkGaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q, K, and V for first head:\\n\")\n",
        "\n",
        "print(f\"q0 {q0.shape}:\\n\", q0, \"\\n\")\n",
        "print(f\"k0 {k0.shape}:\\n\", k0, \"\\n\")\n",
        "print(f\"v0 {v0.shape}:\\n\", v0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQYOlM3kkIsU",
        "outputId": "16744f63-82e0-4b27-d072-283a33f1ea71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q, K, and V for first head:\n",
            "\n",
            "q0 (1, 3, 4):\n",
            " [[[1.42 0.36 0.45 1.61]\n",
            "  [1.6  0.89 0.94 1.79]\n",
            "  [1.19 0.49 0.44 1.03]]] \n",
            "\n",
            "k0 (1, 3, 4):\n",
            " [[[1.98 1.47 0.76 1.12]\n",
            "  [1.61 1.83 0.91 1.5 ]\n",
            "  [1.36 1.04 0.74 0.98]]] \n",
            "\n",
            "v0 (1, 3, 4):\n",
            " [[[0.98 0.85 1.   0.68]\n",
            "  [1.35 0.92 0.85 0.79]\n",
            "  [0.48 0.85 0.53 0.65]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "  key_dim = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  scaled_scores = tf.matmul(query, key, transpose_b=True) / np.sqrt(key_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
        "\n",
        "  softmax = tf.keras.layers.Softmax()\n",
        "  weights = softmax(scaled_scores) \n",
        "  return tf.matmul(weights, value), weights"
      ],
      "metadata": {
        "id": "uEOedj2OqAdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out0, attn_weights0 = scaled_self_attention(q0, k0, v0)\n",
        "\n",
        "print(\"Output from first attention head: \", out0, \"\\n\")\n",
        "print(\"Attention weights from first head: \", attn_weights0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "SUPN8epUkK0V",
        "outputId": "294a6197-07a0-4897-9ccd-6bec84238496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-98ce7d93a6c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_self_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output from first attention head: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attention weights from first head: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-dcee5b4c017e>\u001b[0m in \u001b[0;36mscaled_self_attention\u001b[0;34m(query, key, value)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscaled_self_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mkey_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mQK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;31m#/np.sqrt(key_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (1,3,4) and (4,3,1) not aligned: 4 (dim 2) != 3 (dim 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "seq_len = 3\n",
        "embed_dim = 12\n",
        "num_heads = 3\n",
        "head_dim = embed_dim // num_heads"
      ],
      "metadata": {
        "id": "OZbSK_I-QD8o"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to overcome problems with numpy dot product for batch\n",
        "## create torch array\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "print(\"Input shape: \", x.shape, \"\\n\")\n",
        "print(\"Input:\\n\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yuCrBgu1QwD",
        "outputId": "5c4775e0-9102-47b7-9139-0b5747a7c196"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  torch.Size([1, 3, 12]) \n",
            "\n",
            "Input:\n",
            " tensor([[[ 0.2619, -1.5973,  0.2629,  0.0645,  0.2112, -0.4840,  0.4545,\n",
            "           0.2482,  1.9125, -0.5893,  0.7240,  1.7931],\n",
            "         [ 1.5193,  0.6065, -1.8665,  1.1290, -1.0805,  0.1319,  1.2407,\n",
            "          -0.2018, -0.3274, -1.0611,  0.3008, -0.0039],\n",
            "         [ 0.3538,  0.7198, -0.4635,  1.8290,  0.4108, -0.5122,  0.1376,\n",
            "          -1.3006,  0.0455,  0.6456, -1.1356,  1.3295]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wq0 = torch.rand(embed_dim, head_dim)\n",
        "wq1 = torch.rand(embed_dim, head_dim)\n",
        "wq2 = torch.rand(embed_dim, head_dim)\n",
        "\n",
        "# The key weights for each head. \n",
        "wk0 = torch.rand(embed_dim, head_dim)\n",
        "wk1 = torch.rand(embed_dim, head_dim)\n",
        "wk2 = torch.rand(embed_dim, head_dim)\n",
        "\n",
        "# The value weights for each head.\n",
        "wv0 = torch.rand(embed_dim, head_dim)\n",
        "wv1 = torch.rand(embed_dim, head_dim)\n",
        "wv2 = torch.rand(embed_dim, head_dim)"
      ],
      "metadata": {
        "id": "N4pkyHch1wid"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wq0.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4qdjpu_2bHQ",
        "outputId": "54360c06-6f1e-47ca-f90e-e7e14a2952f1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.matmul(x,wq0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc5gtPbqF-Gu",
        "outputId": "e5456ad9-698c-40b4-ef8a-0bdebe711093"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.6788,  1.7449, -0.4234,  0.7830],\n",
              "         [-0.5399,  0.5887, -0.0268, -1.2599],\n",
              "         [ 2.4524,  2.3112,  1.5315,  0.1854]]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.einsum('bij,jk->bik', x, wq0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFT-w36Z2L2w",
        "outputId": "11915ce0-a1c8-4e45-97b8-28a8da746665"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.6788,  1.7449, -0.4234,  0.7830],\n",
              "         [-0.5399,  0.5887, -0.0268, -1.2599],\n",
              "         [ 2.4524,  2.3112,  1.5315,  0.1854]]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_function = nn.Softmax(dim=1)\n",
        "input = torch.randn(2, 3)\n",
        "print(input)\n",
        "output = softmax_function(input)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMyJ-AwyNZC_",
        "outputId": "98c6bc53-4acd-49af-e9c5-0766af016e6b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1177,  1.2033,  0.1428],\n",
            "        [ 1.5842, -2.2718, -0.4053]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1654, 0.6199, 0.2147],\n",
              "        [0.8636, 0.0183, 0.1181]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_self_attention_torch(query, key, value):\n",
        "  key_dim = key.shape[1]\n",
        "  QK = torch.div(torch.matmul(query,key.T),np.sqrt(key_dim))\n",
        "  print(\"---\")\n",
        "  print(QK.shape)\n",
        "  #sf = nn.Softmax( dim=1)\n",
        "  scaled_QK = softmax_function(QK)\n",
        "  print(scaled_QK.shape)\n",
        "  QKV = torch.matmul(scaled_QK, value)\n",
        "  print(QKV.shape)\n",
        "  return QKV, scaled_QK"
      ],
      "metadata": {
        "id": "y74xrI-wkPMc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Geneated queries, keys, and values for the first head.\n",
        "q0 = torch.matmul(x, wq0)\n",
        "k0 = torch.matmul(x, wk0)\n",
        "v0 = torch.matmul(x, wv0)\n",
        "\n",
        "# Geneated queries, keys, and values for the second head.\n",
        "q1 = torch.matmul(x, wq1)\n",
        "k1 = torch.matmul(x, wk1)\n",
        "v1 = torch.matmul(x, wv1)\n",
        "\n",
        "# Geneated queries, keys, and values for the third head.\n",
        "q2 = torch.matmul(x, wq2)\n",
        "k2 = torch.matmul(x, wk2)\n",
        "v2 = torch.matmul(x, wv2)"
      ],
      "metadata": {
        "id": "mCnyKS72PjiV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q, K, and V for first head:\\n\")\n",
        "\n",
        "print(f\"q0 {q0.shape}:\\n\", q0, \"\\n\")\n",
        "print(f\"k0 {k0.shape}:\\n\", k0, \"\\n\")\n",
        "print(f\"v0 {v0.shape}:\\n\", v0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFIA897APt8q",
        "outputId": "1aaf2075-9803-448d-ac21-84377477bb33"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q, K, and V for first head:\n",
            "\n",
            "q0 torch.Size([1, 3, 4]):\n",
            " tensor([[[ 0.6788,  1.7449, -0.4234,  0.7830],\n",
            "         [-0.5399,  0.5887, -0.0268, -1.2599],\n",
            "         [ 2.4524,  2.3112,  1.5315,  0.1854]]]) \n",
            "\n",
            "k0 torch.Size([1, 3, 4]):\n",
            " tensor([[[ 1.4471,  0.4132,  1.3002,  1.5290],\n",
            "         [-0.7078,  1.5979, -1.0892, -0.1337],\n",
            "         [-1.7239,  0.2325,  0.2377,  0.6269]]]) \n",
            "\n",
            "v0 torch.Size([1, 3, 4]):\n",
            " tensor([[[ 1.3706,  1.3796,  1.2966,  1.5465],\n",
            "         [-0.9009,  0.9481,  0.7197,  1.5377],\n",
            "         [ 0.1803,  1.6086,  0.4795,  1.4874]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out0, attn_weights0 = scaled_self_attention_torch(q0, k0, v0)\n",
        "\n",
        "print(\"Output from first attention head: \", out0, \"\\n\")\n",
        "print(\"Attention weights from first head: \", attn_weights0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "j2Rpbk-RPO8S",
        "outputId": "00208bdf-7459-49bc-a19f-9cb6c6da4a04"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-331783bca393>:3: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3277.)\n",
            "  QK = torch.div(torch.matmul(query,key.T),np.sqrt(key_dim))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-bfac84fc7cc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_self_attention_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output from first attention head: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attention weights from first head: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-331783bca393>\u001b[0m in \u001b[0;36mscaled_self_attention_torch\u001b[0;34m(query, key, value)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscaled_self_attention_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mkey_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mQK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [4, 4] but got: [4, 3]."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYth47s_UceD",
        "outputId": "6355787d-5d93-438a-c0c9-6cc4c2c60467"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.6788,  1.7449, -0.4234,  0.7830],\n",
              "         [-0.5399,  0.5887, -0.0268, -1.2599],\n",
              "         [ 2.4524,  2.3112,  1.5315,  0.1854]]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IezKm_IqUfE3",
        "outputId": "ab00ee98-9de5-4629-d28e-2474d60a81a0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.4471,  0.4132,  1.3002,  1.5290],\n",
              "         [-0.7078,  1.5979, -1.0892, -0.1337],\n",
              "         [-1.7239,  0.2325,  0.2377,  0.6269]]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k0.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lqiodsYUhEE",
        "outputId": "c0690e82-8e52-47dc-a139-454ada7b4fda"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.4471],\n",
              "         [-0.7078],\n",
              "         [-1.7239]],\n",
              "\n",
              "        [[ 0.4132],\n",
              "         [ 1.5979],\n",
              "         [ 0.2325]],\n",
              "\n",
              "        [[ 1.3002],\n",
              "         [-1.0892],\n",
              "         [ 0.2377]],\n",
              "\n",
              "        [[ 1.5290],\n",
              "         [-0.1337],\n",
              "         [ 0.6269]]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.einsum('bij,bkj->bik', q0, k0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQbPr2OeSicv",
        "outputId": "b4629eda-a805-4aa2-af81-19b1fe79b23e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.3498,  2.6644, -0.3742],\n",
              "         [-2.4992,  1.5205,  0.2715],\n",
              "         [ 6.7784,  0.2647, -3.2098]]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Changing our self attention function\n",
        "\n",
        "def scaled_self_attention_torch(query, key, value):\n",
        "  key_dim = key.shape[1]\n",
        "  QK = torch.div(torch.einsum('bij,bkj->bik',query,key),np.sqrt(key_dim))\n",
        "  print(\"---\")\n",
        "  print(QK.shape)\n",
        "  #sf = nn.Softmax( dim=1)\n",
        "  scaled_QK = softmax_function(QK)\n",
        "  print(scaled_QK.shape)\n",
        "  QKV = torch.matmul(scaled_QK, value)\n",
        "  print(QKV.shape)\n",
        "  return QKV, scaled_QK"
      ],
      "metadata": {
        "id": "5ebVqGk7SDC0"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out0, attn_weights0 = scaled_self_attention_torch(q0, k0, v0)\n",
        "\n",
        "print(\"Output from first attention head: \\n\", out0, \"\\n\")\n",
        "print(\"Attention weights from first head: \\n\", attn_weights0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m-EINxPUwZy",
        "outputId": "21a41a04-a23d-4465-b1d8-443d0a33b66e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "torch.Size([1, 3, 3])\n",
            "torch.Size([1, 3, 3])\n",
            "torch.Size([1, 3, 4])\n",
            "Output from first attention head: \n",
            " tensor([[[-0.3435,  1.2433,  0.6814,  1.5432],\n",
            "         [-0.1585,  1.1657,  0.4791,  1.2723],\n",
            "         [ 1.1521,  1.5272,  1.3352,  1.7561]]]) \n",
            "\n",
            "Attention weights from first head: \n",
            " tensor([[[0.0717, 0.5660, 0.3779],\n",
            "         [0.0044, 0.2924, 0.5486],\n",
            "         [0.9240, 0.1416, 0.0735]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zw33IwAEUy5M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}